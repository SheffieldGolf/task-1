{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.readers.CT20_AR_reader import readTweets, readLabels, readTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in tweets, labels & topics using helper functions\n",
    "tweets = readTweets()\n",
    "labels = readLabels()\n",
    "topics = readTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "tweets_df = labels\n",
    "tweets_df['full_text_ar'] = 0\n",
    "\n",
    "for tweet in tweets:\n",
    "    # For each tweet ID, find row with that ID in DataFrame and add Arabic text to that row\n",
    "    tweet_idx = tweets_df[tweets_df['tweetID'] == tweet['id']].index[0]\n",
    "    tweets_df.iloc[tweet_idx, tweets_df.columns.get_loc('full_text_ar')] = tweet['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topicID</th>\n",
       "      <th>tweetID</th>\n",
       "      <th>label</th>\n",
       "      <th>full_text_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221585936095555584</td>\n",
       "      <td>0</td>\n",
       "      <td>Ø¨ÙŠÙ† Ø«ÙˆØ±Ø© Ù„Ø¨Ù†Ø§Ù† Ø§Ù„Ù¡Ù Ù  ÙŠÙˆÙ… ÙˆØ¨ÙŠÙ† Ù¤Ù  Ø§Ù„Ø¹Ø±Ø§Ù‚ \\nØªØªØ± ÙˆØ±ÙØ¹Ø§Ø¹ Ø§ÙŠØ±Ø§Ù† Ù…Ø§ Ø²Ø§Ù„Øª Ø±Ø§Ø¦Ø­Ø© Ø§Ù„Ø¨Ø§Ø±ÙˆØ¯ ÙˆØ§Ù„Ù†Ø§Ø± ØªÙÙˆØ­ Ù…Ù†Ù‡Ù… ØŒ Ù…Ù† Ø­Ø±Ù‚ Ø§Ù„Ø®ÙŠÙ… ÙˆØ¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø±ØµØ§Øµ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØ¸Ø§Ù‡Ø±ÙŠÙ† \\nÙˆØ£ÙƒØªØ§ÙÙ‡Ù… ØªØ¹Ø¨Øª Ù…Ù† Ø¨Ù†Ø§Ø¡ Ø­ÙˆØ§Ø¬Ø² Ø¨ÙŠÙ†Ù‡Ù… ÙˆØ¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø·Ù†ÙŠÙ† Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù…Ù„ÙƒØªÙ‡Ù… Ø§Ù„Ù‡Ø´Ø© \\n\\nØ±ÙØ¹Ø§Ø¹ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø­Ø§Ù„ÙŠ \\n#Ø§ÙŠØ±Ø§Ù† #Ø§Ù„Ø¹Ø±Ø§Ù‚ #Ù„Ø¨Ù†Ø§Ù† \\n#Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221587916750753793</td>\n",
       "      <td>0</td>\n",
       "      <td>#Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ†\\nØ£Ø±Ù…Ù„Ø© ÙÙ‚ÙŠØ±Ø©\\nØ¹Ù„ÙŠÙ‡Ø§Ø¥ÙŠÙ‚Ø§Ù Ø®Ø¯Ù…Ø§Øª\\nØ®Ù„ÙˆÙ†Ø§Ù†ÙØ±Ø­Ù‡Ø§ÙˆØ¹ÙŠØ§Ù„Ù‡Ø§\\nÙ„ÙŠØ³ Ù„Ù‡Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ù„Ù‡ Ø³Ø¨Ø­Ø§Ù†Ù‡ Ø¥Ù„Ø§Ø£Ù†ØªÙ…\\nØ¨ÙŠØª Ø£Ø¬Ø§Ø±ÙˆØ¹Ø§ÙŠØ´Ù‡ Ø¹ Ø§Ù„Ø¶Ù…Ø§Ù†\\nÙˆÙØ§ØªÙˆØ±Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø£Ù‡Ù… ÙˆØ£Ù‚Ù„ Ø´ÙŠ Ø³Ø¯Ø§Ø¯ Ù†Øµ Ø§Ù„ÙØ§ØªÙˆØ±Ø©\\nÙˆØ§Ù„ÙØµÙ„ Ø¨Ø¹Ø¯ÙŠÙˆÙ…\\nØ£Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ ÙŠÙØ±Ø¬ Ù‡Ù…Ù‡Ù…\\nÙˆØ§Ù„Ù„Ù‡ Ù…Ø¨Ù„ØºÙ‡Ù… Ù…Ùˆ Ù…Ø¹Ø¬Ø²Ø©\\nØ§Ù„ØªÙ†ÙÙŠØ°Ø›2030633080(77Ø£Ù„Ù)\\nØ§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡Ø›10097517557(7Ø¢Ù„Ø§Ù) https://t.co/1kzQxf2XeB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221602993918894081</td>\n",
       "      <td>0</td>\n",
       "      <td>#Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø±\\nØ§Ù„Ø­ÙŠØªØ§Ù† Ø§Ù„Ø­ÙŠØ·Ø§Ù†..Ø¹Ù…Ø±ÙˆØ§ Ø¨ÙŠÙ†Ù† Ùˆ Ø¨ÙŠÙ† Ø§Ù„Ø´Ø¹Ø¨..Ø­ÙŠØ·Ø§Ù†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221653116036304896</td>\n",
       "      <td>0</td>\n",
       "      <td>Ù‡Ù„ ÙŠØ­Ù…ÙŠ #Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø± Ø§Ù„Ø³Ù„Ø·Ø© Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ© Ø§Ù„ÙØ§Ø³Ø¯Ø© Ù…Ù† Ø´Ø¹Ø¨Ù‡Ø§ Ø§Ù„Ø«Ø§Ø¦Ø± Ø¶Ø¯Ù‡Ø§ØŸ https://t.co/HYQnmpI07L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221663019345874944</td>\n",
       "      <td>1</td>\n",
       "      <td>Ù…ÙˆØ§Ø²Ù†Ø© Ø´ÙˆØŸ!\\nÙ¡- Ø§Ø±Ù‚Ø§Ù… ØºÙŠØ± ØµØ­ÙŠØ­Ø©.\\nÙ¢- Ø­ÙƒÙˆÙ…Ø© Ù„Ù… ØªÙ†Ù„ Ø§Ù„Ø«Ù‚Ø© Ø¨Ø¹Ø¯ØŒ ÙŠØ¹Ù†ÙŠ ØºÙŠØ± Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ø§Ø±Ù‚Ø§Ù… ÙˆØ¶Ø¹ØªÙ‡Ø§ Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.\\nØ§Ù†Ù‘Ùˆ Ø¬Ù„Ø³Ø© Ø§Ø³ØªÙØ²Ø§Ø² Ù„Ù„Ø´Ø¹Ø¨ØŸ!\\nØ§Ø°Ø§ Ø¨Ø¯ÙƒÙ† Ø§Ù„Ù†Ø§Ø³ ØªØ¹Ø·ÙŠÙƒÙ… ÙØ±ØµØ©ØŒ Ø¨Ø§Ù„Ù‚Ù„ÙŠÙ„Ø© ÙƒÙˆÙ†ÙˆØ§ Ù…Ù†Ø·Ù‚ÙŠÙŠÙ†!\\n#Ù„Ø¨Ù†Ø§Ù†_ÙŠÙ†ØªÙØ¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221688210834759680</td>\n",
       "      <td>0</td>\n",
       "      <td>#Ø³Ù†Ø¯Ø±ÙŠÙ„Ø§Øª_Ù…Ø±Ù‡Ø¬ :ØªØ²Ø§Ù…Ù†Ø§Ù‹ Ù…Ø¹ Ø§Ù†Ø¹Ù‚Ø§Ø¯ Ø¬Ù„Ø³Ø© #Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ù†ÙŠØ§Ø¨ÙŠØ© Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø§Ø«Ù†ÙŠÙ†ØŒ ØªØ§Ø¨Ø¹ÙˆÙ†Ø§ (Ø¥Ù† Ø´Ø¦ØªÙ…) Ø¹Ø¨Ø± Ø´Ø§Ø´Ø© NBN Ø§Ù„Ø³Ø§Ø¹Ø©  Ù¡Ù  ÙˆÙ†ØµÙ ØµØ¨Ø§Ø­Ø§ ÙÙŠ Ø­Ø¯ÙŠØ« Ø³ÙŠØ§Ø³ÙŠ Ù…Ø¨Ø§Ø´Ø± Ø­ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ¬Ø¯Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø§Ø­ØªÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ø¯ÙˆÙ„ÙŠØ© Ø¨Ø¶ÙŠØ§ÙØ© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø£Ù…Ù„ ÙØ¶ÙˆÙ„ ğŸ™‹â€â™€ï¸\\n@nbntweets https://t.co/Tk5frOg2gh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topicID              tweetID  label  \\\n",
       "0  CT20-AR-05  1221585936095555584  0       \n",
       "1  CT20-AR-05  1221587916750753793  0       \n",
       "2  CT20-AR-05  1221602993918894081  0       \n",
       "3  CT20-AR-05  1221653116036304896  0       \n",
       "4  CT20-AR-05  1221663019345874944  1       \n",
       "5  CT20-AR-05  1221688210834759680  0       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                 full_text_ar  \n",
       "0  Ø¨ÙŠÙ† Ø«ÙˆØ±Ø© Ù„Ø¨Ù†Ø§Ù† Ø§Ù„Ù¡Ù Ù  ÙŠÙˆÙ… ÙˆØ¨ÙŠÙ† Ù¤Ù  Ø§Ù„Ø¹Ø±Ø§Ù‚ \\nØªØªØ± ÙˆØ±ÙØ¹Ø§Ø¹ Ø§ÙŠØ±Ø§Ù† Ù…Ø§ Ø²Ø§Ù„Øª Ø±Ø§Ø¦Ø­Ø© Ø§Ù„Ø¨Ø§Ø±ÙˆØ¯ ÙˆØ§Ù„Ù†Ø§Ø± ØªÙÙˆØ­ Ù…Ù†Ù‡Ù… ØŒ Ù…Ù† Ø­Ø±Ù‚ Ø§Ù„Ø®ÙŠÙ… ÙˆØ¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø±ØµØ§Øµ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØ¸Ø§Ù‡Ø±ÙŠÙ† \\nÙˆØ£ÙƒØªØ§ÙÙ‡Ù… ØªØ¹Ø¨Øª Ù…Ù† Ø¨Ù†Ø§Ø¡ Ø­ÙˆØ§Ø¬Ø² Ø¨ÙŠÙ†Ù‡Ù… ÙˆØ¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ§Ø·Ù†ÙŠÙ† Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù…Ù„ÙƒØªÙ‡Ù… Ø§Ù„Ù‡Ø´Ø© \\n\\nØ±ÙØ¹Ø§Ø¹ Ø§Ù„Ù‚Ø±Ù† Ø§Ù„Ø­Ø§Ù„ÙŠ \\n#Ø§ÙŠØ±Ø§Ù† #Ø§Ù„Ø¹Ø±Ø§Ù‚ #Ù„Ø¨Ù†Ø§Ù† \\n#Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ†                                  \n",
       "1  #Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ†\\nØ£Ø±Ù…Ù„Ø© ÙÙ‚ÙŠØ±Ø©\\nØ¹Ù„ÙŠÙ‡Ø§Ø¥ÙŠÙ‚Ø§Ù Ø®Ø¯Ù…Ø§Øª\\nØ®Ù„ÙˆÙ†Ø§Ù†ÙØ±Ø­Ù‡Ø§ÙˆØ¹ÙŠØ§Ù„Ù‡Ø§\\nÙ„ÙŠØ³ Ù„Ù‡Ù… Ø¨Ø¹Ø¯ Ø§Ù„Ù„Ù‡ Ø³Ø¨Ø­Ø§Ù†Ù‡ Ø¥Ù„Ø§Ø£Ù†ØªÙ…\\nØ¨ÙŠØª Ø£Ø¬Ø§Ø±ÙˆØ¹Ø§ÙŠØ´Ù‡ Ø¹ Ø§Ù„Ø¶Ù…Ø§Ù†\\nÙˆÙØ§ØªÙˆØ±Ø© Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø£Ù‡Ù… ÙˆØ£Ù‚Ù„ Ø´ÙŠ Ø³Ø¯Ø§Ø¯ Ù†Øµ Ø§Ù„ÙØ§ØªÙˆØ±Ø©\\nÙˆØ§Ù„ÙØµÙ„ Ø¨Ø¹Ø¯ÙŠÙˆÙ…\\nØ£Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ ÙŠÙØ±Ø¬ Ù‡Ù…Ù‡Ù…\\nÙˆØ§Ù„Ù„Ù‡ Ù…Ø¨Ù„ØºÙ‡Ù… Ù…Ùˆ Ù…Ø¹Ø¬Ø²Ø©\\nØ§Ù„ØªÙ†ÙÙŠØ°Ø›2030633080(77Ø£Ù„Ù)\\nØ§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡Ø›10097517557(7Ø¢Ù„Ø§Ù) https://t.co/1kzQxf2XeB  \n",
       "2  #Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø±\\nØ§Ù„Ø­ÙŠØªØ§Ù† Ø§Ù„Ø­ÙŠØ·Ø§Ù†..Ø¹Ù…Ø±ÙˆØ§ Ø¨ÙŠÙ†Ù† Ùˆ Ø¨ÙŠÙ† Ø§Ù„Ø´Ø¹Ø¨..Ø­ÙŠØ·Ø§Ù†                                                                                                                                                                                                                                                                 \n",
       "3  Ù‡Ù„ ÙŠØ­Ù…ÙŠ #Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø± Ø§Ù„Ø³Ù„Ø·Ø© Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ© Ø§Ù„ÙØ§Ø³Ø¯Ø© Ù…Ù† Ø´Ø¹Ø¨Ù‡Ø§ Ø§Ù„Ø«Ø§Ø¦Ø± Ø¶Ø¯Ù‡Ø§ØŸ https://t.co/HYQnmpI07L                                                                                                                                                                                                                                   \n",
       "4  Ù…ÙˆØ§Ø²Ù†Ø© Ø´ÙˆØŸ!\\nÙ¡- Ø§Ø±Ù‚Ø§Ù… ØºÙŠØ± ØµØ­ÙŠØ­Ø©.\\nÙ¢- Ø­ÙƒÙˆÙ…Ø© Ù„Ù… ØªÙ†Ù„ Ø§Ù„Ø«Ù‚Ø© Ø¨Ø¹Ø¯ØŒ ÙŠØ¹Ù†ÙŠ ØºÙŠØ± Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† Ø§Ø±Ù‚Ø§Ù… ÙˆØ¶Ø¹ØªÙ‡Ø§ Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.\\nØ§Ù†Ù‘Ùˆ Ø¬Ù„Ø³Ø© Ø§Ø³ØªÙØ²Ø§Ø² Ù„Ù„Ø´Ø¹Ø¨ØŸ!\\nØ§Ø°Ø§ Ø¨Ø¯ÙƒÙ† Ø§Ù„Ù†Ø§Ø³ ØªØ¹Ø·ÙŠÙƒÙ… ÙØ±ØµØ©ØŒ Ø¨Ø§Ù„Ù‚Ù„ÙŠÙ„Ø© ÙƒÙˆÙ†ÙˆØ§ Ù…Ù†Ø·Ù‚ÙŠÙŠÙ†!\\n#Ù„Ø¨Ù†Ø§Ù†_ÙŠÙ†ØªÙØ¶                                                                                                                 \n",
       "5  #Ø³Ù†Ø¯Ø±ÙŠÙ„Ø§Øª_Ù…Ø±Ù‡Ø¬ :ØªØ²Ø§Ù…Ù†Ø§Ù‹ Ù…Ø¹ Ø§Ù†Ø¹Ù‚Ø§Ø¯ Ø¬Ù„Ø³Ø© #Ø§Ù„Ù…ÙˆØ§Ø²Ù†Ø© Ø§Ù„Ù†ÙŠØ§Ø¨ÙŠØ© Ø§Ù„ÙŠÙˆÙ… Ø§Ù„Ø§Ø«Ù†ÙŠÙ†ØŒ ØªØ§Ø¨Ø¹ÙˆÙ†Ø§ (Ø¥Ù† Ø´Ø¦ØªÙ…) Ø¹Ø¨Ø± Ø´Ø§Ø´Ø© NBN Ø§Ù„Ø³Ø§Ø¹Ø©  Ù¡Ù  ÙˆÙ†ØµÙ ØµØ¨Ø§Ø­Ø§ ÙÙŠ Ø­Ø¯ÙŠØ« Ø³ÙŠØ§Ø³ÙŠ Ù…Ø¨Ø§Ø´Ø± Ø­ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ¬Ø¯Ø§Øª Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø§Ø­ØªÙŠÙ† Ø§Ù„Ù…Ø­Ù„ÙŠØ© ÙˆØ§Ù„Ø¯ÙˆÙ„ÙŠØ© Ø¨Ø¶ÙŠØ§ÙØ© Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø£Ù…Ù„ ÙØ¶ÙˆÙ„ ğŸ™‹â€â™€ï¸\\n@nbntweets https://t.co/Tk5frOg2gh                                                   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Arabic text pre-processing: (using Farasa api)\n",
    "\n",
    ">1. remove url.\n",
    "2. diacritization.\n",
    "3. tokenisation.\n",
    "4. lemmanisation.\n",
    "5. remove stop words.\n",
    "6. remove punctuation.\n",
    "7. remove emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "url_pattern = r\"https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Farasa API.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizer(text):\n",
    "    import http.client # import http.client in function to avoid connected time out.\n",
    "    import json\n",
    "    text = text.replace('\\n', ' ').replace('\"', '') # otherwise JSONDecodeError.\n",
    "    conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\")\n",
    "    payload = \"{\\\"text\\\":\\\"%s\\\"}\"% text\n",
    "    payload = payload.encode('utf-8')\n",
    "    headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\", }\n",
    "    conn.request(\"POST\", \"/msa/webapi/segmenter\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode('utf-8')\n",
    "    data_dict = json.loads(data)\n",
    "    return data_dict['segtext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(text):\n",
    "    import http.client\n",
    "    import json\n",
    "    text = text.replace('\\n', ' ').replace('\"', '') \n",
    "    conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\")\n",
    "    payload = \"{\\\"text\\\":\\\"%s\\\"}\"% text\n",
    "    payload = payload.encode('utf-8')\n",
    "    headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\", }\n",
    "    conn.request(\"POST\", \"/msa/webapi/lemma\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode('utf-8')\n",
    "    data_dict = json.loads(data)\n",
    "    return data_dict['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diacritizer(text):\n",
    "    import http.client\n",
    "    import json\n",
    "    text = text.replace('\\n', ' ').replace('\"', '') \n",
    "    conn = http.client.HTTPSConnection(\"farasa-api.qcri.org\")\n",
    "    payload = \"{\\\"text\\\":\\\"%s\\\"}\"% text\n",
    "    payload = payload.encode('utf-8')\n",
    "    headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\", }\n",
    "    conn.request(\"POST\", \"/msa/webapi/diacritizeV2\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read().decode('utf-8')\n",
    "    data_dict = json.loads(data)\n",
    "    return data_dict['output'] # return string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_str(l): # Farasa api accept string as input.\n",
    "    text = str()\n",
    "    for w in l:\n",
    "        text+= w\n",
    "        text+=' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor(df):\n",
    "    for i in range(len(df)):\n",
    "        # 1. remove url.\n",
    "        text = re.sub(url_pattern, \"\", df.loc[i, 'full_text_ar'])\n",
    "        # 2. diacritization.\n",
    "        diac_text = diacritizer(text)\n",
    "        # 3. tokenisation.\n",
    "        token_word = tokenizer(diac_text)\n",
    "        text = list_to_str(token_word)\n",
    "        # 4. Lemmatizer.\n",
    "        token_word = lemmatizer(text)\n",
    "        # 5. remove stop words.\n",
    "        cleaned_words = [word for word in token_word if word not in stopwords.words(\"arabic\")]\n",
    "        # 6. remove punctuation.\n",
    "        characters = [',','â€™', '\\'','.','DBSCAN', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%','-','...','^','{','}']\n",
    "        words_lists = [word for word in cleaned_words if word not in characters]\n",
    "        text = list_to_str(words_lists)\n",
    "        # 7. remove emoji\n",
    "        df.loc[i, 'full_text_ar'] = remove_emoji(text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = text_processor(tweets_df) # 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processed_Arabic_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topicID</th>\n",
       "      <th>tweetID</th>\n",
       "      <th>label</th>\n",
       "      <th>full_text_ar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221585936095555584</td>\n",
       "      <td>0</td>\n",
       "      <td>Ø«ÙˆØ± Ø© Ù„Ø¨Ù†Ø§Ù† Ø§Ù„ ÙŠÙˆÙ… Ùˆ Ø§Ù„ Ø¹Ø±Ø§Ù‚ ØªØªØ± Ùˆ Ø±Ø¹Ø§Ø¹ Ø§ÙŠØ±Ø§Ù† Ø²Ø§Ù„ Øª Ø±Ø§Ø¦Ø­ Ø© Ø§Ù„ Ø¨Ø§Ø±ÙˆØ¯ Ùˆ Ø§Ù„ Ù†Ø§Ø± ÙØ§Ø­ ØŒ Ø­Ø±Ù‚ Ø§Ù„ Ø®ÙŠÙ… Ùˆ Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„ Ø±ØµØ§Øµ Ø§Ù„ Ù…ØªØ¸Ø§Ù‡Ø± ÙŠÙ† Ùˆ ÙƒØªÙ ØªØ¹Ø¨ Øª Ø¨Ù†Ø§Ø¡ Ø­Ø§Ø¬Ø² Ùˆ Ø§Ù„ Ù…ÙˆØ§Ø·Ù† ÙŠÙ† Ù„ Ø­Ù…Ø§ÙŠ Ø© Ù…Ù…Ù„Ùƒ Øª Ø§Ù„ Ù‡Ø´ Ø© Ø±Ø¹Ø§Ø¹ Ø§Ù„ Ù‚Ø±Ù† Ø§Ù„ Ø­Ø§Ù„ #Ø§ÙŠØ±Ø§Ù† #Ø§Ù„Ø¹Ø±Ø§Ù‚ #Ù„Ø¨Ù†Ø§Ù† #Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221587916750753793</td>\n",
       "      <td>0</td>\n",
       "      <td>#Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ† Ø£Ø±Ù…Ù„ Ø© ÙÙ‚ÙŠØ± Ø© Ø¹Ù„ÙŠÙ‡Ø§Ø¥ÙŠÙ‚Ø§Ù Ø®Ø¯Ù… Ø§Øª Ø®Ù„ÙˆÙ†Ø§Ù†ÙØ±Ø­Ù‡Ø§ÙˆØ¹ÙŠØ§Ù„Ù‡Ø§ Ù„ Ø§Ù„Ù„Ù‡ Ø³Ø¨Ø­Ø§Ù† Ù‡ Ø¥Ù„Ø§Ø£Ù†ØªÙ… Ø¨ÙŠØª Ø£Ø¬Ø§Ø±ÙˆØ¹Ø§ÙŠØ´Ù‡ Ø¹ Ø§Ù„ Ø¶Ù…Ø§Ù† Ùˆ ÙØ§ØªÙˆØ± Ø© Ø§Ù„ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø£Ù‡Ù… Ùˆ Ø´ÙŠ Ø³Ø¯Ø§Ø¯ Ù†Øµ Ø§Ù„ ÙØ§ØªÙˆØ± Ø© Ùˆ Ø§Ù„ ÙØµÙ„ Ø¨Ø¹Ø¯ÙŠÙˆÙ… Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ Ø£ÙØ±Ø¬ Ùˆ Ø§Ù„Ù„Ù‡ Ù…Ø¨Ù„Øº Ù…Ùˆ Ù…Ø¹Ø¬Ø² Ø© Ø§Ù„ ØªÙ†ÙÙŠØ° Ø› 2030633080 77 Ø£Ù„Ù Ø§Ù„ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø› 10097517557 7 Ø£Ù„Ù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221602993918894081</td>\n",
       "      <td>0</td>\n",
       "      <td>#Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø± Ø§Ù„ Ø­Ùˆ Øª Ø§Ù„ Ø­ÙŠØ·Ø§Ù† Ø¹Ù…Ø± ÙˆØ§ Ù† Ùˆ Ø§Ù„ Ø´Ø¹Ø¨ Ø­ÙŠØ·Ø§Ù†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT20-AR-05</td>\n",
       "      <td>1221653116036304896</td>\n",
       "      <td>0</td>\n",
       "      <td>Ø­Ù…Ù‰ #Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø± Ø§Ù„ Ø³Ù„Ø· Ø© Ø§Ù„ Ø³ÙŠØ§Ø³ÙŠ Ø© Ø§Ù„ ÙØ§Ø³Ø¯ Ø© Ø´Ø¹Ø¨ Ø§Ù„ Ø«Ø§Ø¦Ø± Ø¶Ø¯ ØŸ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topicID              tweetID  label  \\\n",
       "0  CT20-AR-05  1221585936095555584  0       \n",
       "1  CT20-AR-05  1221587916750753793  0       \n",
       "2  CT20-AR-05  1221602993918894081  0       \n",
       "3  CT20-AR-05  1221653116036304896  0       \n",
       "\n",
       "                                                                                                                                                                                                                                                                             full_text_ar  \n",
       "0  Ø«ÙˆØ± Ø© Ù„Ø¨Ù†Ø§Ù† Ø§Ù„ ÙŠÙˆÙ… Ùˆ Ø§Ù„ Ø¹Ø±Ø§Ù‚ ØªØªØ± Ùˆ Ø±Ø¹Ø§Ø¹ Ø§ÙŠØ±Ø§Ù† Ø²Ø§Ù„ Øª Ø±Ø§Ø¦Ø­ Ø© Ø§Ù„ Ø¨Ø§Ø±ÙˆØ¯ Ùˆ Ø§Ù„ Ù†Ø§Ø± ÙØ§Ø­ ØŒ Ø­Ø±Ù‚ Ø§Ù„ Ø®ÙŠÙ… Ùˆ Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„ Ø±ØµØ§Øµ Ø§Ù„ Ù…ØªØ¸Ø§Ù‡Ø± ÙŠÙ† Ùˆ ÙƒØªÙ ØªØ¹Ø¨ Øª Ø¨Ù†Ø§Ø¡ Ø­Ø§Ø¬Ø² Ùˆ Ø§Ù„ Ù…ÙˆØ§Ø·Ù† ÙŠÙ† Ù„ Ø­Ù…Ø§ÙŠ Ø© Ù…Ù…Ù„Ùƒ Øª Ø§Ù„ Ù‡Ø´ Ø© Ø±Ø¹Ø§Ø¹ Ø§Ù„ Ù‚Ø±Ù† Ø§Ù„ Ø­Ø§Ù„ #Ø§ÙŠØ±Ø§Ù† #Ø§Ù„Ø¹Ø±Ø§Ù‚ #Ù„Ø¨Ù†Ø§Ù† #Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ†                                     \n",
       "1  #Ù„Ø¨Ù†Ø§Ù†_Ø§Ù„Ù†Ø§_Ù…Ø´_Ø§Ù„ÙƒÙ† Ø£Ø±Ù…Ù„ Ø© ÙÙ‚ÙŠØ± Ø© Ø¹Ù„ÙŠÙ‡Ø§Ø¥ÙŠÙ‚Ø§Ù Ø®Ø¯Ù… Ø§Øª Ø®Ù„ÙˆÙ†Ø§Ù†ÙØ±Ø­Ù‡Ø§ÙˆØ¹ÙŠØ§Ù„Ù‡Ø§ Ù„ Ø§Ù„Ù„Ù‡ Ø³Ø¨Ø­Ø§Ù† Ù‡ Ø¥Ù„Ø§Ø£Ù†ØªÙ… Ø¨ÙŠØª Ø£Ø¬Ø§Ø±ÙˆØ¹Ø§ÙŠØ´Ù‡ Ø¹ Ø§Ù„ Ø¶Ù…Ø§Ù† Ùˆ ÙØ§ØªÙˆØ± Ø© Ø§Ù„ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø£Ù‡Ù… Ùˆ Ø´ÙŠ Ø³Ø¯Ø§Ø¯ Ù†Øµ Ø§Ù„ ÙØ§ØªÙˆØ± Ø© Ùˆ Ø§Ù„ ÙØµÙ„ Ø¨Ø¹Ø¯ÙŠÙˆÙ… Ø³Ø£Ù„ Ø§Ù„Ù„Ù‡ Ø£ÙØ±Ø¬ Ùˆ Ø§Ù„Ù„Ù‡ Ù…Ø¨Ù„Øº Ù…Ùˆ Ù…Ø¹Ø¬Ø² Ø© Ø§Ù„ ØªÙ†ÙÙŠØ° Ø› 2030633080 77 Ø£Ù„Ù Ø§Ù„ ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø› 10097517557 7 Ø£Ù„Ù   \n",
       "2  #Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø± Ø§Ù„ Ø­Ùˆ Øª Ø§Ù„ Ø­ÙŠØ·Ø§Ù† Ø¹Ù…Ø± ÙˆØ§ Ù† Ùˆ Ø§Ù„ Ø´Ø¹Ø¨ Ø­ÙŠØ·Ø§Ù†                                                                                                                                                                                                                                    \n",
       "3  Ø­Ù…Ù‰ #Ø¬Ø¯Ø§Ø±_Ø§Ù„Ø¹Ø§Ø± Ø§Ù„ Ø³Ù„Ø· Ø© Ø§Ù„ Ø³ÙŠØ§Ø³ÙŠ Ø© Ø§Ù„ ÙØ§Ø³Ø¯ Ø© Ø´Ø¹Ø¨ Ø§Ù„ Ø«Ø§Ø¦Ø± Ø¶Ø¯ ØŸ                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Aravec:\n",
    "\n",
    "download pre-trained word embedding: https://github.com/bakrianoo/aravec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "from utilities import * # import utilities.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = gensim.models.Word2Vec.load('tweet_cbow_300/tweets_cbow_300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = t_model.wv['Ø§Ù„']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train_mean_X = list()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    train_text = [t_model.wv[x] for x in df.loc[i, 'full_text_ar'].split(' ') if x in t_model]\n",
    "    vector = np.array(train_text).mean(axis=0)\n",
    "    train_mean_X.append(vector)\n",
    "     \n",
    "train_mean_X = np.array(train_mean_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 4)\n",
      "(1500, 300)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "\n",
    "print(train_mean_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train_y = np.array(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "train_mean_X, dev_mean_X, train_y, test_y = train_test_split(train_mean_X, train_y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 300)\n",
      "(450, 300)\n",
      "(1050,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(train_mean_X.shape)\n",
    "print(dev_mean_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean score:  0.7666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_mean = LogisticRegression(random_state=0, solver='lbfgs').fit(train_mean_X, train_y)\n",
    "print(\"mean score: \", clf_mean.score(dev_mean_X, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
