{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Exploration\n",
    "\n",
    "#### **Model 1: Neural Network with Word2Vec Embeddings**\n",
    "\n",
    "Initial attempt at using the stemmed, TFIDF weighted unigrams alongside the Word2Vec embeddings in a simple NN. Can consider POS tags, n-grams, etc. later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2062ce17d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258 words in dict\n"
     ]
    }
   ],
   "source": [
    "# Load Word2Vec embeddings and read into dictionary\n",
    "wv = KeyedVectors.load_word2vec_format('dataset_features/word_embedding_output')\n",
    "wv_dict = dict({})\n",
    "for index, key in enumerate(wv.vocab):\n",
    "    wv_dict[key] = wv[key]\n",
    "\n",
    "print(len(wv_dict), 'words in dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaaa</th>\n",
       "      <th>aaaaaaagel</th>\n",
       "      <th>aaaier</th>\n",
       "      <th>aaar</th>\n",
       "      <th>aaarsat</th>\n",
       "      <th>aabernamha</th>\n",
       "      <th>aabyuhzha</th>\n",
       "      <th>aada</th>\n",
       "      <th>aajabi</th>\n",
       "      <th>...</th>\n",
       "      <th>zlv</th>\n",
       "      <th>zlzlhm</th>\n",
       "      <th>zmrolha</th>\n",
       "      <th>zndqoh</th>\n",
       "      <th>zone</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zti</th>\n",
       "      <th>ztoot</th>\n",
       "      <th>zugheib</th>\n",
       "      <th>zuo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1499 rows × 6115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaaaaa  aaaaaaagel  aaaier  aaar  aaarsat  aabernamha  aabyuhzha  \\\n",
       "0     0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "1     0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "2     0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "3     0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "4     0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "...   ...     ...         ...     ...   ...      ...         ...        ...   \n",
       "1494  0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "1495  0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "1496  0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "1497  0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "1498  0.0     0.0         0.0     0.0   0.0      0.0         0.0        0.0   \n",
       "\n",
       "      aada  aajabi  ...  zlv  zlzlhm  zmrolha  zndqoh  zone  zouk  zti  ztoot  \\\n",
       "0      0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "1      0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "2      0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "3      0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "4      0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "...    ...     ...  ...  ...     ...      ...     ...   ...   ...  ...    ...   \n",
       "1494   0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "1495   0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "1496   0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "1497   0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "1498   0.0     0.0  ...  0.0     0.0      0.0     0.0   0.0   0.0  0.0    0.0   \n",
       "\n",
       "      zugheib  zuo  \n",
       "0         0.0  0.0  \n",
       "1         0.0  0.0  \n",
       "2         0.0  0.0  \n",
       "3         0.0  0.0  \n",
       "4         0.0  0.0  \n",
       "...       ...  ...  \n",
       "1494      0.0  0.0  \n",
       "1495      0.0  0.0  \n",
       "1496      0.0  0.0  \n",
       "1497      0.0  0.0  \n",
       "1498      0.0  0.0  \n",
       "\n",
       "[1499 rows x 6115 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stemmed, TFIDF weighted terms & label/topic for each tweet\n",
    "data = pd.read_csv('tfidf_with_stemming.csv', index_col=0)\n",
    "labels = pd.read_csv('translated_tweets_t1.csv', usecols=['label', 'topicID'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660 words found out of 6115\n"
     ]
    }
   ],
   "source": [
    "# Create matrix of embedding weights\n",
    "\n",
    "embed_dim = len(wv_dict['zone']) # 200, same for all terms\n",
    "matrix_len = len(data.columns) # Number of terms in tweets vocab\n",
    "embed_weights = np.zeros((matrix_len, embed_dim)) # Initialise weights\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(data.columns):\n",
    "    try:\n",
    "        embed_weights[i] = wv_dict[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        embed_weights[i] = np.random.normal(scale=0.6, size=(embed_dim, ))\n",
    "        \n",
    "# Convert embeddings and data to tensor format\n",
    "embed_weights = torch.from_numpy(embed_weights)\n",
    "data = torch.from_numpy(data.to_numpy())\n",
    "\n",
    "print(words_found, 'words found out of', len(embed_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create embeddings layer\n",
    "def create_embedding_layer(embed_weights):\n",
    "    \n",
    "    num_embeddings, embed_dim = embed_weights.shape\n",
    "    embedding_layer = nn.Embedding(num_embeddings, embed_dim)\n",
    "    embedding_layer.load_state_dict({'weight': embed_weights})\n",
    "\n",
    "    return embedding_layer, num_embeddings, embed_dim\n",
    "\n",
    "# Define simple NN model w/ embeddings\n",
    "class SimpleNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_weights, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding, num_embeddings, embed_dims = create_embedding_layer(embed_weights)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embed_weights.shape[0] * embed_weights.shape[1], num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_limit = 0.5\n",
    "        self.embedding.weight.data.uniform_(- init_limit, init_limit)\n",
    "        self.fc.weight.data.uniform_(- init_limit, init_limit)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text.long())\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# Instantiate the model\n",
    "model = SimpleNN(embed_weights, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for insertion into Dataset object\n",
    "\n",
    "labels_tensor = torch.tensor(labels['label'].to_numpy(), dtype=torch.long)\n",
    "entries = []\n",
    "\n",
    "for i, tweet in enumerate(data):\n",
    "    label = labels_tensor[i].item()\n",
    "    weights = tweet\n",
    "    entry = (label, weights)\n",
    "    entries.append(entry)\n",
    "    \n",
    "# Create Dataset object\n",
    "\n",
    "class Tweets(Dataset):\n",
    "    def __init__(self):\n",
    "        self.samples = entries\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "    \n",
    "tweets_dataset = Tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and testing behaviour\n",
    "\n",
    "def train(sub_train_):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    data = DataLoader(sub_train_, batch_size=1, shuffle=True)\n",
    "    for cls, text in data:\n",
    "        optimiser.zero_grad()\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls.long())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    scheduler.step() # Adjust learning rate\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    preds, true = [], []\n",
    "    data = DataLoader(data_, batch_size=1)\n",
    "    for cls, text in data:\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls.long())\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (output.argmax(1) == cls).sum().item()\n",
    "            preds.append(max(output))\n",
    "            true.append(cls)\n",
    "\n",
    "    return test_loss / len(data_), test_acc / len(data_), preds, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 46 seconds\n",
      "\tLoss: 0.6931(train)\t|\tAcc: 30.8%(train)\n",
      "\tLoss: 0.6931(valid)\t|\tAcc: 28.0%(valid)\n",
      "Epoch: 2  | time in 1 minutes, 2 seconds\n",
      "\tLoss: 0.6931(train)\t|\tAcc: 30.8%(train)\n",
      "\tLoss: 0.6931(valid)\t|\tAcc: 28.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "EPOCHS = 2\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.SGD(model.parameters(), lr=3.0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimiser, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(tweets_dataset) * 0.90)\n",
    "sub_train_, sub_valid_ = random_split(tweets_dataset, [train_len, len(tweets_dataset) - train_len])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(sub_train_)\n",
    "    valid_loss, valid_acc, preds, true = test(sub_valid_)\n",
    "    \n",
    "    #map_val = average_precision_score(y_true=true, y_score=preds, average='macro')\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "    #print('Mean Average Precision:', round(map_val, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Poor performance, need to get figure out issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
