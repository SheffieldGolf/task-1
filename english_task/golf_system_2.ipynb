{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English ( LSTM )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114d86790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch   \n",
    "\n",
    "# handling text data.\n",
    "from torchtext import data \n",
    "\n",
    "# set random seed.\n",
    "SEED = 2020\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "from tqdm import tqdm\n",
    "\n",
    "TEXT = data.Field(tokenize=lambda x: x.split(), batch_first=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['since', 'never', 'get', 'report', 'medium', 'want', 'share', 'copy', 'check', 'realdonaldtrump', 'donate', 'salary', 'back', 'united', 'states', 'government', 'â€”', 'quarter', 'donate', 'hhsgov', 'confront', 'contain', 'combat', 'coronavirus', 'flag', 'united', 'statesflag', 'united', 'states'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# choose text and label from dataset. \n",
    "fields = [(None, None), (None, None), (None, None), (None, None), ('text',TEXT), (None, None), ('label', LABEL)]\n",
    "train_data=data.TabularDataset(path='preprocess_train.csv', format='csv',fields=fields, skip_header=True)\n",
    "valid_data=data.TabularDataset(path='preprocess_dev.csv', format='csv',fields=fields, skip_header=True)\n",
    "\n",
    "#print preprocessed text.\n",
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. build vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 741\n",
      "Size of LABEL vocabulary: 2\n",
      "[('coronavirus', 184), ('virus', 124), ('corona', 112), ('covid-19', 110), ('covid19', 104), ('case', 78), ('people', 76), ('test', 65), ('get', 57), ('amp', 55)]\n"
     ]
    }
   ],
   "source": [
    "# initialize glove embeddings.\n",
    "TEXT.build_vocab(train_data, min_freq = 3, vectors = \"glove.6B.300d\")  \n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# vocab.\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "# label.\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "# Commonly used words.\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "# Word dictionary.\n",
    "# print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. define neuronal network architecture:\n",
    "\n",
    "<img src=\"architecture.png\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    \n",
    "    # define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        # Constructor\n",
    "        super().__init__()          \n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        # activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "            #text = [batch size,sent_length]\n",
    "            embedded = self.embedding(text)\n",
    "            #embedded = [batch size, sent_len, emb dim]\n",
    "\n",
    "            #packed sequence\n",
    "            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "\n",
    "            packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "            #hidden = [batch size, num layers * num directions,hid dim]\n",
    "            #cell = [batch size, num layers * num directions,hid dim]\n",
    "\n",
    "            #concat the final forward and backward hidden state\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "\n",
    "            #hidden = [batch size, hid dim * num directions]\n",
    "            dense_outputs=self.fc(hidden)\n",
    "\n",
    "            #Final activation function\n",
    "            outputs=self.act(dense_outputs)\n",
    "\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 1\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers,\n",
    "                   bidirectional = True, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(741, 300)\n",
      "  (lstm): LSTM(300, 32, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "The model has 332,957 trainable parameters\n",
      "torch.Size([741, 300])\n"
     ]
    }
   ],
   "source": [
    "#architecture\n",
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "#push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. define training function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train() \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.text   \n",
    "        \n",
    "        #convert to 1D tensor\n",
    "        predictions = model(text, text_lengths).squeeze()  \n",
    "        \n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.label)        \n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.label)   \n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. define evaluate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    pred_prob_list = list()\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths).squeeze()\n",
    "            \n",
    "            \n",
    "            pred_prob_list.append(predictions)\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), pred_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\tTrain Loss: 0.703 | Train Acc: 46.03%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 51.42%\n",
      "-----------------------------------------------\n",
      "Epoch:  2\n",
      "\tTrain Loss: 0.693 | Train Acc: 45.90%\n",
      "\t Val. Loss: 0.684 |  Val. Acc: 56.11%\n",
      "-----------------------------------------------\n",
      "Epoch:  3\n",
      "\tTrain Loss: 0.682 | Train Acc: 62.04%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 74.20%\n",
      "-----------------------------------------------\n",
      "Epoch:  4\n",
      "\tTrain Loss: 0.668 | Train Acc: 71.22%\n",
      "\t Val. Loss: 0.668 |  Val. Acc: 58.24%\n",
      "-----------------------------------------------\n",
      "Epoch:  5\n",
      "\tTrain Loss: 0.644 | Train Acc: 68.83%\n",
      "\t Val. Loss: 0.648 |  Val. Acc: 59.80%\n",
      "-----------------------------------------------\n",
      "Epoch:  6\n",
      "\tTrain Loss: 0.602 | Train Acc: 74.79%\n",
      "\t Val. Loss: 0.608 |  Val. Acc: 72.68%\n",
      "-----------------------------------------------\n",
      "Epoch:  7\n",
      "\tTrain Loss: 0.528 | Train Acc: 77.79%\n",
      "\t Val. Loss: 0.557 |  Val. Acc: 71.16%\n",
      "-----------------------------------------------\n",
      "Epoch:  8\n",
      "\tTrain Loss: 0.431 | Train Acc: 83.69%\n",
      "\t Val. Loss: 0.528 |  Val. Acc: 73.67%\n",
      "-----------------------------------------------\n",
      "Epoch:  9\n",
      "\tTrain Loss: 0.349 | Train Acc: 86.34%\n",
      "\t Val. Loss: 0.526 |  Val. Acc: 75.71%\n",
      "-----------------------------------------------\n",
      "Epoch:  10\n",
      "\tTrain Loss: 0.265 | Train Acc: 91.23%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 71.12%\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc, pred_prob = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "        # prob of best model.\n",
    "        prob_list = list()\n",
    "        prob_list = pred_prob # 64 by 64 by 20.\n",
    "    \n",
    "    print('Epoch: ', epoch+1)\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model:\n",
    "# path = 'saved_weights.pt'\n",
    "# model.load_state_dict(torch.load(path))\n",
    "# model.eval()\n",
    "\n",
    "# evaluate(model, iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "   \n",
    "preds_proba = list(np.array(prob_list[0])) + list(np.array(prob_list[1])) + list(np.array(prob_list[2]))\n",
    "\n",
    "len(preds_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dev = pd.read_csv('preprocess_dev.csv', usecols=['tweet_id'])\n",
    "\n",
    "dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>score</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235714275752267776</td>\n",
       "      <td>0.461557</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235256530728972290</td>\n",
       "      <td>0.513492</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235648554338791427</td>\n",
       "      <td>0.840732</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235674258858061825</td>\n",
       "      <td>0.882272</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235663306246860800</td>\n",
       "      <td>0.909752</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235436227140055040</td>\n",
       "      <td>0.914788</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235602629247537154</td>\n",
       "      <td>0.849051</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235566351093137408</td>\n",
       "      <td>0.856797</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235620307534258176</td>\n",
       "      <td>0.351739</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235758466784014337</td>\n",
       "      <td>0.033685</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>covid-19</td>\n",
       "      <td>1235956474955718656</td>\n",
       "      <td>0.857291</td>\n",
       "      <td>Model_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_id             tweet_id     score   run_id\n",
       "0   covid-19  1235714275752267776  0.461557  Model_1\n",
       "1   covid-19  1235256530728972290  0.513492  Model_1\n",
       "2   covid-19  1235648554338791427  0.840732  Model_1\n",
       "3   covid-19  1235674258858061825  0.882272  Model_1\n",
       "4   covid-19  1235663306246860800  0.909752  Model_1\n",
       "5   covid-19  1235436227140055040  0.914788  Model_1\n",
       "6   covid-19  1235602629247537154  0.849051  Model_1\n",
       "7   covid-19  1235566351093137408  0.856797  Model_1\n",
       "8   covid-19  1235620307534258176  0.351739  Model_1\n",
       "9   covid-19  1235758466784014337  0.033685  Model_1\n",
       "10  covid-19  1235956474955718656  0.857291  Model_1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['topic_id', 'tweet_id', 'score', 'run_id'])\n",
    "results['tweet_id'] = list(dev['tweet_id'])\n",
    "results['score'] = [x for x in preds_proba]\n",
    "results['topic_id'] = 'covid-19'\n",
    "results['run_id'] = 'Model_1'\n",
    "\n",
    "results.loc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('golf_system_results_2.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO : ================ RESULTS for golf_system_results_2.tsv ====================================\n",
    "\n",
    "INFO : AVERAGE PRECISION:            0.5540    \n",
    "INFO : ==================================================================================\n",
    "\n",
    "INFO : RECIPROCAL RANK:              1.0000    \n",
    "INFO : ===================================================================================\n",
    "\n",
    "INFO : R-PRECISION (R=72):           0.5278    \n",
    "INFO : ===================================================================================\n",
    "\n",
    "INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       \n",
    "INFO :                               1.0000    0.6667    0.6000    0.5000    0.5500    0.6000    \n",
    "INFO : ===================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
